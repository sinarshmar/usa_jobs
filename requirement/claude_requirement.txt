Tasman Senior Data Engineer Assessment - Action Plan
Overview
Total Estimated Time: 6-8 hours Strategy: Build incrementally, showing end-to-end capability first, then add production features
 
Phase 1: Local MVP (2 hours)
Goal: Demonstrate core ETL functionality works end-to-end
Tasks:
    1      Project Setup (15 min)
                ◦       [ ] Create Git repository
            ◦           [ ] Initialize Python project structure
            ◦           [ ] Create virtual environment
            ◦           [ ] Add .gitignore, requirements.txt
            ◦           [ ] Create initial README with project description
    2      API Exploration (30 min)
                ◦       [ ] Test USAJobs API with curl/Postman
            ◦           [ ] Understand response structure
            ◦           [ ] Identify pagination requirements
            ◦           [ ] Document API quirks/limitations
            ◦           [ ] Create sample response JSON for testing
    3      Simple ETL Script (45 min) # src/etl.py
4  - [ ] Basic API connection with API key
     5    - [ ] Search for "data engineering" jobs
     6    - [ ] Parse required fields (PositionTitle, URI, Location, Remuneration)
     7    - [ ] Handle first page only initially
     8    - [ ] Print results to console
            9          
    10    Local Database Integration (30 min)
                ◦       [ ] Create docker-compose.yml with Postgres
            ◦           [ ] Design simple schema (single table initially)
            ◦           [ ] Add database connection code
            ◦           [ ] Insert records (handle duplicates with ON CONFLICT)
            ◦           [ ] Verify data with SELECT query
Deliverables:
    •       Working Python script that fetches and stores data
            •           Local Postgres with job data
            •           Basic project structure
Success Criteria:
python src/etl.py
# Should fetch jobs and insert into local database
docker-compose exec postgres psql -U user -d jobs -c "SELECT COUNT(*) FROM job_listings;"
# Should show inserted records

Phase 2: Production Hardening (1.5 hours)
Goal: Add production-ready features and error handling
Tasks:
    1      Code Refactoring (30 min)
                ◦       [ ] Separate concerns into modules:
          ▪  api_client.py - USAJobs API interaction
     ▪    db.py - Database operations
     ▪    config.py - Configuration management
     ▪    models.py - Data models/schemas
                ◦       [ ] Add environment variable support
            ◦           [ ] Create config for local vs cloud
    2      Error Handling & Resilience (30 min)
                ◦       [ ] Add retry logic with exponential backoff
            ◦           [ ] Handle API rate limits gracefully
            ◦           [ ] Add connection error handling
            ◦           [ ] Implement transaction rollback on failure
            ◦           [ ] Add logging (structured JSON logs)
    3      Data Handling Improvements (30 min)
                ◦       [ ] Handle pagination properly
            ◦           [ ] Add data validation (required fields)
            ◦           [ ] Implement upsert logic (update if exists)
            ◦           [ ] Add ETL metadata (timestamp, version)
            ◦           [ ] Handle missing/null fields gracefully
Deliverables:
    •       Modular, maintainable code structure
            •           Comprehensive error handling
            •           Production-ready logging
Corner Cases Handled:
    •       ✅ API timeout/connection errors
            •           ✅ Duplicate job listings
            •           ✅ Missing fields in response
            •           ✅ Database connection failures
            •           ✅ Partial page processing
 
Phase 3: Containerization (1 hour)
Goal: Package application for cloud deployment
Tasks:
    1      Docker Setup (30 min)
                ◦       [ ] Create multi-stage Dockerfile
            ◦           [ ] Optimize for size (alpine base)
            ◦           [ ] Add health check
            ◦           [ ] Handle signals properly (SIGTERM)
            ◦           [ ] Create .dockerignore
    2      Docker Compose Enhancement (15 min)
                ◦       [ ] Add ETL service to docker-compose
            ◦           [ ] Configure networking between services
            ◦           [ ] Add volume for persistent data
            ◦           [ ] Environment file for secrets
    3      Testing (15 min)
                ◦       [ ] Build and run container locally
            ◦           [ ] Verify connectivity to Postgres
            ◦           [ ] Test with docker-compose up
            ◦           [ ] Document container resource requirements
Deliverables:
    •       Dockerfile optimized for production
            •           Complete docker-compose for local testing
            •           Container documentation
Validation:
docker build -t tasman-etl .
docker-compose up
# Everything should work in containers

Phase 4: Cloud Deployment Setup (1.5 hours)
Goal: Prepare for cloud deployment without incurring costs
Tasks:
    1      Free Cloud Database (30 min)
                ◦       [ ] Sign up for Supabase/Neon free tier
            ◦           [ ] Create database and schema
            ◦           [ ] Get connection string
            ◦           [ ] Test connection from local
            ◦           [ ] Update config for cloud DB option
    2      Cloud Run Preparation (30 min)
                ◦       [ ] Adapt code for Cloud Run Jobs
            ◦           [ ] Add Cloud Run service YAML
            ◦           [ ] Configure memory/CPU limits
            ◦           [ ] Add environment variables config
            ◦           [ ] Create startup/shutdown handlers
    3      Infrastructure as Code (30 min)
                ◦       [ ] Write Terraform/Pulumi configuration
            ◦           [ ] Define Cloud Run Job
            ◦           [ ] Configure Cloud Scheduler
            ◦           [ ] Set up Artifact Registry
            ◦           [ ] Add IAM roles/permissions
            ◦           [ ] Include secret management
Deliverables:
    •       IaC files (Terraform/Pulumi)
            •           Cloud deployment configuration
            •           Deployment instructions
Smart Move:
# In README:
"IaC provided for GCP deployment. Tested once successfully 
then torn down to avoid costs. Can be deployed with:
terraform apply"

Phase 5: Testing & Documentation (1 hour)
Goal: Make it easy for evaluators to run and understand
Tasks:
    1      Testing Suite (20 min)
                ◦       [ ] Unit tests for key functions
            ◦           [ ] Integration test with mock API
            ◦           [ ] Add test data/fixtures
            ◦           [ ] Document test coverage
    2      Comprehensive Documentation (40 min)
                ◦       [ ] README with clear setup instructions
            ◦           [ ] Architecture decision records (ADRs)
            ◦           [ ] API documentation
            ◦           [ ] Deployment guide
            ◦           [ ] Troubleshooting section
            ◦           [ ] List of potential improvements
Documentation Structure:
README.md
- Quick Start (2 commands to run locally)
- Architecture Overview (with diagram)
- Design Decisions (with trade-offs)
- Deployment Options
- Future Improvements
- Known Limitations

Phase 6: Polish & Advanced Features (1 hour) [IF TIME ALLOWS]
Goal: Show senior-level thinking
Optional Enhancements:
    1      Monitoring & Observability
                ◦       [ ] Add metrics collection
            ◦           [ ] Create simple dashboard mockup
            ◦           [ ] Add alerting rules
            ◦           [ ] Performance benchmarks
    2      Advanced Features
                ◦       [ ] Incremental loading (track last_modified)
            ◦           [ ] Data quality checks
            ◦           [ ] Schema migration support
            ◦           [ ] Dry-run mode
            ◦           [ ] Backfill capability
    3      Operational Excellence
                ◦       [ ] Runbook for common issues
            ◦           [ ] Rollback procedure
            ◦           [ ] Canary deployment config
            ◦           [ ] Feature flags
 
Time Management Strategy
Minimum Viable Submission (4 hours):
    •       Phase 1 + 2 + 3 + Basic Documentation
            •           Shows working end-to-end solution
Good Submission (6 hours):
    •       Phase 1-5 complete
            •           Production-ready with cloud deployment ready
Excellent Submission (8 hours):
    •       All phases including optional features
            •           Shows deep senior-level thinking
 
Key Success Metrics
    •       ✅ It works - Can be run locally with minimal setup
            •           ✅ It's production-ready - Handles errors, logs properly
            •           ✅ It's deployable - IaC provided, containerized
            •           ✅ It's maintainable - Clean code, documented decisions
            •           ✅ It shows seniority - Considers edge cases, scalability
 
Risk Mitigation
    1      If running out of time: Focus on documentation of what you would do
            2          If API is difficult: Create mock data, note in README
            3          If cloud costs concern: Test locally, provide IaC without deploying
            4          If stuck: Document the issue and your approach to solving it
 
Git Commit Strategy
# Commit after each phase completion
git commit -m "Phase 1: Basic ETL with local database"
git commit -m "Phase 2: Add error handling and resilience"
git commit -m "Phase 3: Containerize application"
git commit -m "Phase 4: Add cloud deployment configuration"
git commit -m "Phase 5: Complete documentation and testing"
This shows progressive development and thinking process.
 
Final Checklist Before Submission
    •       [ ] Code runs with single docker-compose command
            •           [ ] README has clear instructions
            •           [ ] IaC files are present (even if not deployed)
            •           [ ] Design decisions are documented
            •           [ ] Future improvements are listed
            •           [ ] Repository is shared with specified users
